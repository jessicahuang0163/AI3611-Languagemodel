meta_data:
  pipeline_exp_name: "benchmark"
  description: "the complete pipeline of language model"
  num_workers: 1
  use_gpu: true
  key_config:
    common:
      model_name: "model"
      nhid: "nhid"
      nlayers: "nlayers"
      emsize: "emb"
      nhead: "nhead"

common:
  model_name: 'MemTransformer'  # 'type of network (RNN_TANH, RNN_RELU, LSTM, GRU, Transformer)'
  nhid: 550  # number of hidden units per layer
  nlayers: 2  # number of layers
  emsize: 550  # size of word embeddings
  nhead: 2  # the number of heads in the encoder/decoder of the transformer model

algo_training:
  variables:
    lr: [10]  
    clip: [0.05]  # gradient clipping
    epochs: [40]
    batch_size: [30]
    bptt: [110]  # sequence length
    dropout: [0.2]
    seed: [1111]

  constants:
    exp_name: "memtransformer_train"  # exp_name is required
    script_path: algo_exp_script.py
    log_interval: 500  # report interval
    save: 'model.pt'  # path to save the final model
    onnx_export: ''  # path to export the final model in onnx format
    tied: False  # tie the word embedding and softmax weights
    dry_run: False  # verify the code and the model

